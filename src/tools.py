import numpy as np
import librosa
from moviepy.editor import *
import os
from PIL import Image


def get_audio_file(folder):
    # ÐŸÑƒÑ‚ÑŒ Ðº Ð°ÑƒÐ´Ð¸Ð¾-Ñ„Ð°Ð¹Ð»Ñƒ
    audio_file = [os.path.join(folder, music) \
                    for music in os.listdir(folder) if music.endswith(('.mp3'))][0] # os.path.join(folder, "music.mp3")

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ Ð»Ð¸ Ð°ÑƒÐ´Ð¸Ð¾-Ñ„Ð°Ð¹Ð»
    if not os.path.isfile(audio_file):
        print(f"âŒAudio file not found in {folder}")
        return
    
    return audio_file    

def get_directory_from_path(file_path):
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÑƒÑ‚ÑŒ Ðº Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
    directory_path = os.path.dirname(file_path)
    return directory_path

def get_filename_without_extension(file_path):
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¸Ð¼Ñ Ñ„Ð°Ð¹Ð»Ð° Ñ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸ÐµÐ¼
    file_name_with_extension = os.path.basename(file_path)
    # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ
    file_name_without_extension = os.path.splitext(file_name_with_extension)[0]
    return file_name_without_extension

def suggest_frequency_bands(audio_file, num_bands=4, sr=None, n_fft=2048, hop_length=None):
    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð°ÑƒÐ´Ð¸Ð¾ Ñ„Ð°Ð¹Ð»
    y, sr = librosa.load(audio_file, sr=sr, mono=True)

    if hop_length is None:
        hop_length = n_fft // 4

    # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÑÐ¿ÐµÐºÑ‚Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ
    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))

    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
    frequencies = librosa.fft_frequencies(sr=sr, n_fft=n_fft)

    # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð°Ð¼Ð¿Ð»Ð¸Ñ‚ÑƒÐ´Ñ‹ Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ (ÑÐ¿ÐµÐºÑ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ„Ð»ÑŽÐºÑ)
    spectral_flux = np.var(S, axis=1)

    # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÐµÐºÑ‚Ñ€ Ð½Ð° Ñ€Ð°Ð²Ð½Ñ‹Ðµ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»Ñ‹
    num_freqs = len(frequencies)
    freq_bins = np.linspace(frequencies[0], frequencies[-1], num=500)

    # Ð¡ÑƒÐ¼Ð¼Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¿ÐµÐºÑ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ„Ð»ÑŽÐºÑ Ð² ÐºÐ°Ð¶Ð´Ð¾Ð¼ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð½Ð¾Ð¼ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ðµ
    flux_per_band = []
    for i in range(len(freq_bins) - 1):
        freq_mask = (frequencies >= freq_bins[i]) & (frequencies < freq_bins[i+1])
        if np.any(freq_mask):
            flux = np.sum(spectral_flux[freq_mask])
            flux_per_band.append((flux, freq_bins[i], freq_bins[i+1]))

    # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ñ‹ Ð¿Ð¾ Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð½Ðµ ÑÐ¿ÐµÐºÑ‚Ñ€Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ„Ð»ÑŽÐºÑÐ° (Ð¸Ð·Ð¼ÐµÐ½Ñ‡Ð¸Ð²Ð¾ÑÑ‚Ð¸)
    flux_per_band.sort(reverse=True, key=lambda x: x[0])

    # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ñ‚Ð¾Ð¿ `num_bands` Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ð¾Ð²
    top_bands = flux_per_band[:num_bands]

    # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ñ‹ Ð¿Ð¾ Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚Ð°Ð½Ð¸ÑŽ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹
    top_bands.sort(key=lambda x: x[1])

    # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ð¾Ð²
    suggested_bands = [(round(band[1]), round(band[2])) for band in top_bands]

    print("ÐŸÑ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÐ¼Ñ‹Ðµ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð½Ñ‹Ðµ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ñ‹:")
    for idx, band in enumerate(suggested_bands):
        print(f"Ð”Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½ {idx+1}: {band[0]} - {band[1]} Ð“Ñ†")

    return suggested_bands


def merge_videos_with_audio(video_files, audio_file, output_file):
    """
    ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ ÑÐ¿Ð¸ÑÐ¾Ðº Ð²Ð¸Ð´ÐµÐ¾Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¸ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ Ðº Ð½Ð¸Ð¼ Ð°ÑƒÐ´Ð¸Ð¾, Ð·Ð°Ñ‚ÐµÐ¼ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð² Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ Ñ„Ð°Ð¹Ð».
    
    :param video_files: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¿ÑƒÑ‚ÐµÐ¹ Ðº Ð²Ð¸Ð´ÐµÐ¾Ñ„Ð°Ð¹Ð»Ð°Ð¼ (Ð±ÐµÐ· Ð°ÑƒÐ´Ð¸Ð¾).
    :param audio_file: ÐŸÑƒÑ‚ÑŒ Ðº Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ð°Ð¹Ð»Ñƒ (MP3).
    :param output_file: ÐŸÑƒÑ‚ÑŒ Ðº Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð²Ð¸Ð´ÐµÐ¾Ñ„Ð°Ð¹Ð»Ñƒ.
    """
    print(f"ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ {len(video_files)} Ð²Ð¸Ð´ÐµÐ¾Ñ„Ð°Ð¹Ð»Ð¾Ð².")
    
    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð²Ð¸Ð´ÐµÐ¾Ñ„Ð°Ð¹Ð»Ñ‹
    clips = []
    for idx, file in enumerate(video_files):
        print(f"ðŸ“‚ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð²Ð¸Ð´ÐµÐ¾Ñ„Ð°Ð¹Ð»Ð° {idx + 1}/{len(video_files)}: {file}")
        clip = VideoFileClip(file)
        clips.append(clip)
    
    print("âœ… Ð’ÑÐµ Ð²Ð¸Ð´ÐµÐ¾Ñ„Ð°Ð¹Ð»Ñ‹ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹.")

    # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð²Ð¸Ð´ÐµÐ¾Ñ„Ð°Ð¹Ð»Ñ‹
    print("ðŸ”— ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð²Ð¸Ð´ÐµÐ¾Ñ„Ð°Ð¹Ð»Ð¾Ð²...")
    final_clip = concatenate_videoclips(clips, method="compose")
    print("ðŸŽ¬ Ð’Ð¸Ð´ÐµÐ¾ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¾.")

    # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð°ÑƒÐ´Ð¸Ð¾ Ñ„Ð°Ð¹Ð»
    print(f"ðŸŽµ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ð°Ð¹Ð»Ð°: {audio_file}")
    audio = AudioFileClip(audio_file)
    print("âœ… ÐÑƒÐ´Ð¸Ð¾Ñ„Ð°Ð¹Ð» ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½.")

    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð°ÑƒÐ´Ð¸Ð¾ Ðº Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¼Ñƒ Ð²Ð¸Ð´ÐµÐ¾
    print("ðŸŽ¶ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð°ÑƒÐ´Ð¸Ð¾ Ðº Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð½Ð¾Ð¼Ñƒ Ð²Ð¸Ð´ÐµÐ¾...")
    final_clip = final_clip.set_audio(audio)
    print("âœ… ÐÑƒÐ´Ð¸Ð¾ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾ Ðº Ð²Ð¸Ð´ÐµÐ¾.")

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»
    print(f"ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð¾Ð³Ð¾ Ñ„Ð°Ð¹Ð»Ð°: {output_file}")
    final_clip.write_videofile(output_file, codec="libx264", preset="ultrafast", threads=4)
    print(f"ðŸŽ‰ Ð¤Ð°Ð¹Ð» ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½: {output_file}")


def merge_videos(output_file, video_files):
    
    # folder = 'path/to/your/videos'
    # video_files = sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.mp4')])

    # # Output file path
    # output_file = os.path.join(folder, "merged_output_video.mp4")
    
    # Load all video clips
    clips = [VideoFileClip(file) for file in video_files]
    
    # Concatenate all clips into one
    final_clip = concatenate_videoclips(clips, method="compose")
    
    # Write the result to a file
    final_clip.write_videofile(output_file, codec="libx264", threads=4)


def get_segment_duration(total_duration, segment_number, total_segments):
    # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°
    segment_length = total_duration // total_segments
    
    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð½Ð°Ñ‡Ð°Ð»Ð¾ Ð¸ ÐºÐ¾Ð½ÐµÑ† ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°
    start_time = segment_number * segment_length
    end_time = start_time + segment_length
    
    # Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚, ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ¾Ð½ÐµÑ‡Ð½Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ
    if segment_number == total_segments + 1:
        end_time = total_duration - 1
    
    return start_time, end_time

def add_gif(gif_file, audio_duration, slideshow):
    if gif_file and os.path.isfile(gif_file):
        print("âœ”Gif file found")

        has_mask = False # has_transparency(gif_file)
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ GIF Ð¸ Ð·Ð°Ñ†Ð¸ÐºÐ»Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ð²ÑÑŽ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾
        gif_clip = (VideoFileClip(gif_file, has_mask)
                    .loop(duration=(14.0 * 2.0)) # two times # duration=audio_duration)
                    # .resize(0.5)  # ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ (0.5 = 50% Ð¾Ñ‚ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð°)
                    .set_position(("left", "bottom")))  # ÐŸÐ¾Ð·Ð¸Ñ†Ð¸Ñ (Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð½Ð° Ð½ÑƒÐ¶Ð½ÑƒÑŽ)

        # Ð”ÐµÐ»Ð°ÐµÐ¼ Ñ„Ð¾Ð½ GIF Ð¿Ñ€Ð¾Ð·Ñ€Ð°Ñ‡Ð½Ñ‹Ð¼ (ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ñ‹Ð¹ Ñ†Ð²ÐµÑ‚)
        gif_clip = gif_clip.fx(vfx.mask_color, color=[0, 0, 0], thr=100, s=5)

        inspect_clip("gif_clip", gif_clip)

        # ÐÐ°ÐºÐ»Ð°Ð´Ñ‹Ð²Ð°ÐµÐ¼ GIF Ð¿Ð¾Ð²ÐµÑ€Ñ… ÑÐ»Ð°Ð¹Ð´-ÑˆÐ¾Ñƒ
        final_video = CompositeVideoClip([slideshow, gif_clip])
    else:
        final_video = slideshow
    return final_video

def create_slideshow_with_fade_OLD(images, audio_duration, image_duration=2, fade_duration=0.1):
    image_clips = []
    
    # Create individual image clips with fade in and fade out
    for img in images:
        clip = ImageClip(img).set_duration(image_duration)
        # Apply fade in and fade out effect
        clip = clip.crossfadein(fade_duration).crossfadeout(fade_duration)
        image_clips.append(clip)

    # Concatenate images to form a slideshow
    slideshow = concatenate_videoclips(image_clips, method="compose")

    # Loop the slideshow to match the audio duration
    print(f"SLIDESðŸ–¼ :: ðŸ”Looping at âŒ›duration: {audio_duration}")
    looped_slideshow = slideshow.loop(duration=audio_duration)

    return looped_slideshow

from moviepy.editor import ImageClip, concatenate_videoclips

def create_slideshow_with_fade(images, audio_duration, image_duration=2, fade_duration=0.1):
    image_clips = []
    
    print(f"SLIDESðŸ–¼ :: Total images: {len(images)} :: ðŸ”Looping at âŒ›duration: {audio_duration} secs")
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ»Ð¸Ð¿Ñ‹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð°Ð¼Ð¸ Ð·Ð°Ñ‚ÐµÐ¼Ð½ÐµÐ½Ð¸Ñ Ð¸ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ñ
    for img in images:
        clip = ImageClip(img).set_duration(image_duration)
        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ ÑÑ„Ñ„ÐµÐºÑ‚Ñ‹ Ð¿Ð»Ð°Ð²Ð½Ð¾Ð³Ð¾ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ñ Ð¸ Ð¸ÑÑ‡ÐµÐ·Ð½Ð¾Ð²ÐµÐ½Ð¸Ñ
        clip = clip.fadein(fade_duration).fadeout(fade_duration)
        image_clips.append(clip)
    
    # ÐšÐ¾Ð½ÐºÐ°Ñ‚ÐµÐ½Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² ÑÐ»Ð°Ð¹Ð´ÑˆÐ¾Ñƒ
    slideshow = concatenate_videoclips(image_clips, method="compose")
    
    # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑÐ»Ð°Ð¹Ð´ÑˆÐ¾Ñƒ
    slideshow_duration = slideshow.duration
    
    # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼, ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ€Ð°Ð· Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¸Ñ‚ÑŒ ÑÐ»Ð°Ð¹Ð´ÑˆÐ¾Ñƒ
    num_repeats = int(audio_duration // slideshow_duration) + 1
    
    # Ð Ð°Ð·Ð¼Ð½Ð¾Ð¶Ð°ÐµÐ¼ Ð¼Ð°ÑÑÐ¸Ð² ÐºÐ»Ð¸Ð¿Ð¾Ð²
    replicated_clips = image_clips * num_repeats
    print(f"SLIDESðŸ–¼ :: Replicated images: {len(replicated_clips)} :: ðŸ”Repeated: {num_repeats} times")
    
    # ÐšÐ¾Ð½ÐºÐ°Ñ‚ÐµÐ½Ð¸Ñ€ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð¼Ð½Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ðµ ÐºÐ»Ð¸Ð¿Ñ‹ Ð² Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐ»Ð°Ð¹Ð´ÑˆÐ¾Ñƒ
    full_slideshow = concatenate_videoclips(replicated_clips, method="compose")
    
    # ÐžÐ±Ñ€ÐµÐ·Ð°ÐµÐ¼ ÑÐ»Ð°Ð¹Ð´ÑˆÐ¾Ñƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð½Ð¾ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¾Ð²Ð°Ð»Ð¾ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð°ÑƒÐ´Ð¸Ð¾
    final_slideshow = full_slideshow.subclip(0, audio_duration)
    
    return final_slideshow



def inspect_clip(name, clip, debug=False):
    if not debug:
        return
    
    # Get the size (resolution) of the clip
    print(f"===== Inspect clip: {name} ========")
    size = clip.size
    print(f"Size (width, height): {size}")
    
    # Get the duration of the clip
    duration = clip.duration
    print(f"Duration: {duration} seconds")
    
    # Check if the clip has an alpha mask (transparency)
    has_transparency = clip.mask is not None
    print(f"Has transparency (alpha channel): {has_transparency}")
    
    # Get the number of color channels by inspecting a frame
    frame = clip.get_frame(0)  # Get the first frame of the clip
    num_channels = frame.shape[2] if len(frame.shape) == 3 else 1  # Check if the frame has color channels
    print(f"Number of color channels: {num_channels} (â„¹: 3 for RGB, 4 for RGBA)")
    
    # Print if the clip has transparency based on number of channels
    if num_channels == 4:
        print("This clip is RGBA (has transparency).")
    else:
        print("This clip is RGB (no transparency).")
    print("==============================")

# Example usage with a clip:
# Assuming you have a clip object
# inspect_clip(clip)


def has_transparency(gif_path):
    img = Image.open(gif_path)
    if img.mode in ('RGBA', 'LA') or (img.mode == 'P' and 'transparency' in img.info):
        return True
    return False


# # Assuming 'final_video' is your video clip and 'audio_duration' is the duration of the audio
# text = "Ð’ÐžÐ¢ Ð‘Ð« Ð‘Ð«Ð›Ð"  # The text you want to add

# # Add text overlay to the video
# final_video_with_text = add_text_overlay(final_video, text, duration=audio_duration)

# # Save the final video with text
# final_video_with_text.write_videofile("output_with_text.mp4", fps=24)

def add_text_overlay(video_clip, text, duration, fontsize=50, color='white', transparency=0.6):
    # Create the text clip
    text_clip = (TextClip(text, fontsize=fontsize, color=color, font='Arial-Bold')
                 .set_duration(duration)
                 .set_position(('center', 'bottom'))  # Position at the center bottom of the video
                 .set_opacity(transparency))  # Set transparency

    # Overlay the text on the video
    video_with_text = CompositeVideoClip([video_clip, text_clip])

    return video_with_text